{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ma9fvn89zl",
   "metadata": {},
   "source": [
    "# PPO Trading Agent\n",
    "\n",
    "This notebook trains a **Proximal Policy Optimization (PPO)** agent to trade stocks using reinforcement learning.\n",
    "\n",
    "## Trading Approach: Close-to-Open\n",
    "\n",
    "We use a **close-to-open** strategy:\n",
    "\n",
    "| Time | Action |\n",
    "|------|--------|\n",
    "| **Day T close** | Agent observes all Day T data (OHLCV, indicators) |\n",
    "| **Day T close** | Agent decides position for overnight |\n",
    "| **Day T+1 open** | Trade executes at next day's open price |\n",
    "\n",
    "**Why this works without shifting features:**\n",
    "- At market close, all of Day T's data is known (Open, High, Low, Close, Volume)\n",
    "- Decision is made after close, so no lookahead bias\n",
    "- Position is held overnight, captured in next day's open-to-close move\n",
    "\n",
    "This is realistic for:\n",
    "- EOD (end-of-day) trading systems\n",
    "- Swing trading strategies\n",
    "- Any strategy that doesn't require intraday execution\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "1. **Data & Diagnostics** - Load features and check predictiveness\n",
    "2. **Environment Setup** - Configure trading simulation with reward shaping\n",
    "3. **Training** - Train PPO with tuned hyperparameters\n",
    "4. **Evaluation** - Test on held-out data with comprehensive metrics\n",
    "5. **Feature Selection** - Ablation study to find best features\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6571157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/pnl1f276/code/bennystu/trend_surgeon/Trend-Surgeon-Time-Series/ts_boilerplate\")\n",
    "\n",
    "from gym_anytrading.envs.flexible_env import FlexibleTradingEnv\n",
    "from dataprep import build_feature_dataset, get_X_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xcva2ymrdsf",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Build the feature dataset. All features use **no shifting** since we're using a close-to-open approach where decisions are made after market close with full knowledge of the day's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9301b6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Integrity Report ---\n",
      "Rows: 2,891, Columns: 33\n",
      "Index sorted: True\n",
      "Index unique: True\n",
      "Rows that are entirely NaN: 0\n",
      "Inf values: 0\n",
      "\n",
      "Top columns with NaNs:\n",
      "  (None)\n",
      "\n",
      "--- Data Integrity Report ---\n",
      "Rows: 2,891, Columns: 33\n",
      "Index sorted: True\n",
      "Index unique: True\n",
      "Rows that are entirely NaN: 0\n",
      "Inf values: 0\n",
      "\n",
      "Top columns with NaNs:\n",
      "  (None)\n",
      "\n",
      "Markdown feature documentation written to: ../docs/feature_documentation.md\n"
     ]
    }
   ],
   "source": [
    "df = build_feature_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azlfud5g2",
   "metadata": {},
   "source": [
    "## 3. Feature Predictiveness Diagnostic\n",
    "\n",
    "**Run this first!** Tests if today's features can predict tomorrow's price direction.\n",
    "\n",
    "- If test accuracy ≈ 50%, features have no predictive power → RL won't work\n",
    "- Look at feature importances to identify what matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0e3d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting next-day direction (Up/Down)\n",
      "Train: 2312, Test: 578\n",
      "Base rate: Train=51.2% up, Test=52.2% up\n",
      "\n",
      "Model                     Train Acc    Test Acc     Test AUC     Verdict\n",
      "----------------------------------------------------------------------\n",
      "Logistic Regression         54.8%       49.8%      0.501       No signal\n",
      "Random Forest               73.0%       51.9%      0.541       No signal\n",
      "Random Forest               73.0%       51.9%      0.541       No signal\n",
      "Gradient Boosting           79.8%       51.7%      0.533       No signal\n",
      "\n",
      "TOP 10 FEATURES:\n",
      "  XPH_Return_1d                  0.0623 ██████\n",
      "  XPH_Close                      0.0455 ████\n",
      "  PPH_Return_1d                  0.0433 ████\n",
      "  XPH_Ratio_PPH                  0.0431 ████\n",
      "  XPH_Volume                     0.0406 ████\n",
      "  days_to_cpi                    0.0401 ████\n",
      "  days_since_nfp                 0.0391 ███\n",
      "  PPH_Entropy_20                 0.0390 ███\n",
      "  PPH_High                       0.0384 ███\n",
      "  PPH_SMA_10                     0.0376 ███\n",
      "Gradient Boosting           79.8%       51.7%      0.533       No signal\n",
      "\n",
      "TOP 10 FEATURES:\n",
      "  XPH_Return_1d                  0.0623 ██████\n",
      "  XPH_Close                      0.0455 ████\n",
      "  PPH_Return_1d                  0.0433 ████\n",
      "  XPH_Ratio_PPH                  0.0431 ████\n",
      "  XPH_Volume                     0.0406 ████\n",
      "  days_to_cpi                    0.0401 ████\n",
      "  days_since_nfp                 0.0391 ███\n",
      "  PPH_Entropy_20                 0.0390 ███\n",
      "  PPH_High                       0.0384 ███\n",
      "  PPH_SMA_10                     0.0376 ███\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = df.columns.drop(\"target_close\")\n",
    "X = df[feature_cols].values\n",
    "prices_arr = df[\"target_close\"].values\n",
    "\n",
    "# Target: next day direction (1 = up, 0 = down)\n",
    "y = (prices_arr[1:] > prices_arr[:-1]).astype(int)\n",
    "X = X[:-1]\n",
    "\n",
    "# Train/test split (temporal)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Test multiple models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Predicting next-day direction (Up/Down)\")\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "print(f\"Base rate: Train={y_train.mean()*100:.1f}% up, Test={y_test.mean()*100:.1f}% up\\n\")\n",
    "print(f\"{'Model':<25} {'Train Acc':<12} {'Test Acc':<12} {'Test AUC':<12} {'Verdict'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))\n",
    "    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "    try:\n",
    "        test_auc = roc_auc_score(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "    except:\n",
    "        test_auc = 0.5\n",
    "\n",
    "    verdict = \"Promising\" if test_acc > 0.55 and test_auc > 0.55 else \"Weak signal\" if test_acc > 0.52 else \"No signal\"\n",
    "    print(f\"{name:<25} {train_acc*100:>6.1f}%     {test_acc*100:>6.1f}%     {test_auc:>6.3f}       {verdict}\")\n",
    "\n",
    "# Feature importance\n",
    "rf_model = models[\"Random Forest\"]\n",
    "importances = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTOP 10 FEATURES:\")\n",
    "for i, row in importances.head(10).iterrows():\n",
    "    bar = '█' * int(row['importance'] * 100)\n",
    "    print(f\"  {row['feature']:<30} {row['importance']:.4f} {bar}\")\n",
    "\n",
    "feature_importances = importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2q00vs3pgc",
   "metadata": {},
   "source": [
    "## 4. Prepare Features & Prices\n",
    "\n",
    "Extract the target price series and feature matrix for the trading environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e8f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = df[\"target_close\"].values.astype(np.float32)\n",
    "\n",
    "# Remove macro features\n",
    "keywords = [\"cpi\", \"nfp\", \"holiday\"]\n",
    "cols_to_drop = df.columns[df.columns.str.contains(\"|\".join(keywords), case=False)]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Remove target feature (keep column names, not a DataFrame)\n",
    "feature_cols = df.columns.drop(\"target_close\")\n",
    "\n",
    "\n",
    "\n",
    "signal_features = df[feature_cols].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "il8zyenap9h",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split\n",
    "\n",
    "Split data temporally (80/20) to evaluate on unseen future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295b8735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2891\n",
      "Train: (30, 2312) (2282 steps)\n",
      "Test:  (2312, 2891) (579 steps)\n"
     ]
    }
   ],
   "source": [
    "window_size = 30\n",
    "train_ratio = 0.8\n",
    "\n",
    "total_len = len(df)\n",
    "train_end = int(total_len * train_ratio)\n",
    "\n",
    "train_frame_bound = (window_size, train_end)\n",
    "test_frame_bound = (train_end, total_len)\n",
    "\n",
    "print(f\"Total samples: {total_len}\")\n",
    "print(f\"Train: {train_frame_bound} ({train_end - window_size} steps)\")\n",
    "print(f\"Test:  {test_frame_bound} ({total_len - train_end} steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mzqvet6615",
   "metadata": {},
   "source": [
    "## 6. Environment Configuration\n",
    "\n",
    "Configure the `FlexibleTradingEnv` with reward shaping parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `reward_scaling` | Multiplies small log-returns for stronger signal |\n",
    "| `trade_penalty` | Extra cost per trade to reduce churning |\n",
    "| `profit_bonus` | Bonus multiplier for profitable positions |\n",
    "| `trend_following_bonus` | Small reward for aligning with trend |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0865b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Config: {'fee': 0.0005, 'holding_cost': 0.0, 'short_borrow_cost': 0.0, 'reward_scaling': 100.0, 'trade_penalty': 0.001, 'profit_bonus': 0.5, 'trend_following_bonus': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "REWARD_CONFIG = {\n",
    "    'fee': 0.0005,              # 0.05% per trade\n",
    "    'holding_cost': 0.0,\n",
    "    'short_borrow_cost': 0.0,\n",
    "    # Additional shaping parameters (not supported by the env constructor),\n",
    "    # will be applied to the env instance only if the attribute exists.\n",
    "    'reward_scaling': 100.0,    # Scale up tiny log-returns\n",
    "    'trade_penalty': 0.001,     # Reduce churning\n",
    "    'profit_bonus': 0.5,        # 50% bonus on profits\n",
    "    'trend_following_bonus': 0.0001,\n",
    "}\n",
    "\n",
    "print(\"Reward Config:\", REWARD_CONFIG)\n",
    "\n",
    "def make_train_env():\n",
    "    def _init():\n",
    "        # Pass only supported constructor args; do not expand REWARD_CONFIG directly\n",
    "        env = FlexibleTradingEnv(\n",
    "            df=df, prices=prices, signal_features=signal_features,\n",
    "            window_size=window_size, frame_bound=train_frame_bound,\n",
    "            include_position_in_obs=True,\n",
    "            fee=REWARD_CONFIG.get('fee', 0.0005),\n",
    "            holding_cost=REWARD_CONFIG.get('holding_cost', 0.0),\n",
    "            short_borrow_cost=REWARD_CONFIG.get('short_borrow_cost', 0.0),\n",
    "        )\n",
    "        # Apply extra shaping attributes only if the env supports them\n",
    "        for k, v in REWARD_CONFIG.items():\n",
    "            if hasattr(env, k):\n",
    "                setattr(env, k, v)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_test_env():\n",
    "    def _init():\n",
    "        env = FlexibleTradingEnv(\n",
    "            df=df, prices=prices, signal_features=signal_features,\n",
    "            window_size=window_size, frame_bound=test_frame_bound,\n",
    "            include_position_in_obs=True,\n",
    "            fee=REWARD_CONFIG.get('fee', 0.0005),\n",
    "            holding_cost=REWARD_CONFIG.get('holding_cost', 0.0),\n",
    "            short_borrow_cost=REWARD_CONFIG.get('short_borrow_cost', 0.0),\n",
    "        )\n",
    "        for k, v in REWARD_CONFIG.items():\n",
    "            if hasattr(env, k):\n",
    "                setattr(env, k, v)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129rp4rj2w",
   "metadata": {},
   "source": [
    "## 7. Create Vectorized Environments\n",
    "\n",
    "Wrap environments with `VecNormalize` for observation and reward normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e75c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = DummyVecEnv([make_train_env()])\n",
    "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True, clip_reward=np.inf)\n",
    "\n",
    "# Use a separate env for evaluation (do NOT reuse training env factory)\n",
    "eval_callback_env = DummyVecEnv([make_test_env()])\n",
    "eval_callback_env = VecNormalize(eval_callback_env, training=False, norm_obs=True, norm_reward=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5hh4qzw2np",
   "metadata": {},
   "source": [
    "## 8. PPO Model\n",
    "\n",
    "Key hyperparameters tuned for trading:\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| `n_steps` | 2048 | Longer rollouts capture multi-day patterns |\n",
    "| `ent_coef` | 0.05 | Higher exploration (default 0.01) |\n",
    "| `gamma` | 0.95 | Shorter horizon focus for trading |\n",
    "| `net_arch` | [256, 256] | Wider networks for complex patterns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f34049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_env,\n",
    "    learning_rate=lambda progress: 3e-4 * (1 - progress * 0.5),\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    gamma=0.95,\n",
    "    gae_lambda=0.9,\n",
    "    ent_coef=0.05,\n",
    "    clip_range=0.2,\n",
    "    n_epochs=10,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    normalize_advantage=True,\n",
    "    policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])),\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_logs/\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dx1i4aqmcyi",
   "metadata": {},
   "source": [
    "## 9. Training Callbacks\n",
    "\n",
    "- `SyncNormCallback`: Syncs normalization stats between train and eval envs\n",
    "- `EvalCallback`: Periodic evaluation and saves best model\n",
    "- `CheckpointCallback`: Saves model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b03036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class SyncNormCallback(BaseCallback):\n",
    "    \"\"\"Syncs VecNormalize stats from train_env to eval_env.\"\"\"\n",
    "    def __init__(self, train_env, eval_env, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.train_env = train_env\n",
    "        self.eval_env = eval_env\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.eval_env.obs_rms = self.train_env.obs_rms\n",
    "        self.eval_env.ret_rms = self.train_env.ret_rms\n",
    "        return True\n",
    "\n",
    "sync_callback = SyncNormCallback(train_env, eval_callback_env)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_callback_env,\n",
    "    best_model_save_path=\"./ppo_best_model/\",\n",
    "    log_path=\"./ppo_eval_logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,\n",
    "    save_path=\"./ppo_checkpoints/\",\n",
    "    name_prefix=\"ppo_trading_model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1e4xq15yn",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "Training for **50k timesteps** (~2-3 min on M4 Mac). Increase to 200k for better results when ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621de619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_logs/PPO_19\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 5405 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3088        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015789105 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.613      |\n",
      "|    learning_rate        | 0.000152    |\n",
      "|    loss                 | -0.0873     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    value_loss           | 0.0805      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-0.39 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 578       |\n",
      "|    mean_reward          | -0.395    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0212094 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.661    |\n",
      "|    explained_variance   | -0.763    |\n",
      "|    learning_rate        | 0.000153  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0573   |\n",
      "|    value_loss           | 0.0261    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2420 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2431        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027423745 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | -0.0928     |\n",
      "|    learning_rate        | 0.000155    |\n",
      "|    loss                 | -0.121      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0656     |\n",
      "|    value_loss           | 0.0186      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.39 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.394     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03206775 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | 0.00607    |\n",
      "|    learning_rate        | 0.000156   |\n",
      "|    loss                 | -0.149     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0687    |\n",
      "|    value_loss           | 0.0149     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2249  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 4     |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2283        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036794074 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.000158    |\n",
      "|    loss                 | -0.112      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0658     |\n",
      "|    value_loss           | 0.013       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2286        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043969862 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.000159    |\n",
      "|    loss                 | -0.12       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.063      |\n",
      "|    value_loss           | 0.0128      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.411     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04835363 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.525     |\n",
      "|    explained_variance   | 0.617      |\n",
      "|    learning_rate        | 0.000161   |\n",
      "|    loss                 | -0.115     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0605    |\n",
      "|    value_loss           | 0.0114     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2203  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2231        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062528566 |\n",
      "|    clip_fraction        | 0.341       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.000162    |\n",
      "|    loss                 | -0.13       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0686     |\n",
      "|    value_loss           | 0.00706     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-0.42 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.418     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06603132 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.474     |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.000164   |\n",
      "|    loss                 | -0.117     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0646    |\n",
      "|    value_loss           | 0.0059     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2162  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2187       |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06788395 |\n",
      "|    clip_fraction        | 0.327      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.46      |\n",
      "|    explained_variance   | 0.764      |\n",
      "|    learning_rate        | 0.000165   |\n",
      "|    loss                 | -0.121     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0655    |\n",
      "|    value_loss           | 0.00574    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2206       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07108013 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.443     |\n",
      "|    explained_variance   | 0.618      |\n",
      "|    learning_rate        | 0.000167   |\n",
      "|    loss                 | -0.12      |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.00593    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-0.32 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.32      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06917994 |\n",
      "|    clip_fraction        | 0.298      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.419     |\n",
      "|    explained_variance   | 0.195      |\n",
      "|    learning_rate        | 0.000168   |\n",
      "|    loss                 | -0.108     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.00718    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2165  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2183       |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07929227 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.409     |\n",
      "|    explained_variance   | 0.601      |\n",
      "|    learning_rate        | 0.00017    |\n",
      "|    loss                 | -0.0987    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.055     |\n",
      "|    value_loss           | 0.00458    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-0.27 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.27      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09094608 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.401     |\n",
      "|    explained_variance   | 0.61       |\n",
      "|    learning_rate        | 0.000172   |\n",
      "|    loss                 | -0.0927    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 0.00387    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2145  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2162        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099172994 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | 0.625       |\n",
      "|    learning_rate        | 0.000173    |\n",
      "|    loss                 | -0.105      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 0.00464     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2178       |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10421631 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.376     |\n",
      "|    explained_variance   | 0.655      |\n",
      "|    learning_rate        | 0.000175   |\n",
      "|    loss                 | -0.0985    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0584    |\n",
      "|    value_loss           | 0.00387    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-0.41 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 578       |\n",
      "|    mean_reward          | -0.408    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 35000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0920057 |\n",
      "|    clip_fraction        | 0.306     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.372    |\n",
      "|    explained_variance   | 0.639     |\n",
      "|    learning_rate        | 0.000176  |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0525   |\n",
      "|    value_loss           | 0.00412   |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2151  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2163        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098835886 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.000178    |\n",
      "|    loss                 | -0.0998     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0589     |\n",
      "|    value_loss           | 0.00372     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-0.22 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.218     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11158122 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.355     |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 0.000179   |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0551    |\n",
      "|    value_loss           | 0.00409    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2136  |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2150       |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11412142 |\n",
      "|    clip_fraction        | 0.316      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.359     |\n",
      "|    explained_variance   | 0.732      |\n",
      "|    learning_rate        | 0.000181   |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0504    |\n",
      "|    value_loss           | 0.0038     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-0.19 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | -0.194     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 45000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10361704 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.358     |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000182   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.0594    |\n",
      "|    value_loss           | 0.00324    |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2127  |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2127        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097909436 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.000184    |\n",
      "|    loss                 | -0.077      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0556     |\n",
      "|    value_loss           | 0.0039      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 2114       |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10083488 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.361     |\n",
      "|    explained_variance   | 0.595      |\n",
      "|    learning_rate        | 0.000185   |\n",
      "|    loss                 | -0.0922    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0584    |\n",
      "|    value_loss           | 0.00314    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=0.09 +/- 0.00\n",
      "Episode length: 578.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 578        |\n",
      "|    mean_reward          | 0.0868     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12065978 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.353     |\n",
      "|    explained_variance   | 0.808      |\n",
      "|    learning_rate        | 0.000187   |\n",
      "|    loss                 | -0.12      |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0589    |\n",
      "|    value_loss           | 0.00297    |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(\n",
    "    total_timesteps=200_000,\n",
    "    callback=[sync_callback, eval_callback, checkpoint_callback]\n",
    ")\n",
    "\n",
    "model.save(\"ppo_trading_final\")\n",
    "train_env.save(\"vec_normalize.pkl\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fkizuc270xi",
   "metadata": {},
   "source": [
    "## 11. Sanity Check (Train Data)\n",
    "\n",
    "Quick check that the model learned something on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f27950",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_env = DummyVecEnv([make_train_env()])\n",
    "sanity_env = VecNormalize.load(\"vec_normalize.pkl\", sanity_env)\n",
    "sanity_env.training = False\n",
    "sanity_env.norm_reward = False\n",
    "\n",
    "model = PPO.load(\"ppo_trading_final\")\n",
    "\n",
    "obs = sanity_env.reset()\n",
    "done = False\n",
    "profits = []\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = sanity_env.step(action)\n",
    "    profits.append(info[0][\"total_profit\"])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(profits)\n",
    "plt.title(\"Agent Performance (Train Data)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Cumulative Log Return\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vzej7vd1sm",
   "metadata": {},
   "source": [
    "## 12. Evaluation Function\n",
    "\n",
    "Comprehensive evaluation with:\n",
    "- **Equity curves** (net & gross) vs Buy & Hold\n",
    "- **Risk metrics**: Sharpe, Max Drawdown, Calmar\n",
    "- **Trade analysis**: Win rate, profit factor\n",
    "- **Visualizations**: Drawdown chart, position map, trade distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01baca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(model, vec_env, episodes=1, fee=0.0005):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation for a trading env using SB3 + VecNormalize.\n",
    "\n",
    "    Includes:\n",
    "    - Equity curve with transaction costs\n",
    "    - Multiple benchmarks (Buy & Hold, Short & Hold)\n",
    "    - Detailed performance metrics\n",
    "    - Position analysis\n",
    "    - Drawdown visualization\n",
    "    - Trade markers\n",
    "    \"\"\"\n",
    "\n",
    "    # Get raw (true) underlying environment\n",
    "    base_env = vec_env.venv.envs[0].unwrapped\n",
    "    all_prices = base_env.prices.astype(float)\n",
    "    start_tick = base_env._start_tick\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION STARTED\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Environment starts at tick {start_tick}\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        obs = vec_env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Track equity and positions indexed by tick for proper alignment\n",
    "        equity_by_tick = {}      # tick -> equity value (net)\n",
    "        equity_gross_by_tick = {}  # tick -> equity value (gross)\n",
    "        positions_by_tick = {}   # tick -> position\n",
    "\n",
    "        positions = []\n",
    "        ticks = []\n",
    "        actions_taken = []\n",
    "\n",
    "        # Start portfolio at 1.0\n",
    "        current_equity_gross = 1.0\n",
    "        current_equity_net = 1.0\n",
    "        last_position = 0\n",
    "\n",
    "        # Get initial tick after reset\n",
    "        current_tick = base_env._current_tick\n",
    "        ticks.append(current_tick)\n",
    "        equity_by_tick[current_tick] = current_equity_net\n",
    "        equity_gross_by_tick[current_tick] = current_equity_gross\n",
    "        positions_by_tick[current_tick] = 0\n",
    "        positions.append(0)\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = vec_env.step(action)\n",
    "\n",
    "            done = bool(done[0])\n",
    "            info = info[0]\n",
    "\n",
    "            current_tick = info[\"tick\"]\n",
    "            pos = info[\"position\"]\n",
    "\n",
    "            ticks.append(current_tick)\n",
    "            positions.append(pos)\n",
    "            positions_by_tick[current_tick] = pos\n",
    "            actions_taken.append(int(action[0]) if hasattr(action, '__len__') else int(action))\n",
    "\n",
    "            # Update equity using actual price indices\n",
    "            if len(ticks) > 1:\n",
    "                prev_tick = ticks[-2]\n",
    "                price_ratio = all_prices[current_tick] / all_prices[prev_tick]\n",
    "\n",
    "                # Gross equity (no fees)\n",
    "                current_equity_gross *= price_ratio ** last_position\n",
    "\n",
    "                # Net equity (with fees)\n",
    "                current_equity_net *= price_ratio ** last_position\n",
    "                if pos != last_position:\n",
    "                    if last_position == 0:\n",
    "                        current_equity_net *= (1 - fee)\n",
    "                    elif pos == 0:\n",
    "                        current_equity_net *= (1 - fee)\n",
    "                    else:  # flip\n",
    "                        current_equity_net *= (1 - 2*fee)\n",
    "\n",
    "            equity_by_tick[current_tick] = current_equity_net\n",
    "            equity_gross_by_tick[current_tick] = current_equity_gross\n",
    "            last_position = pos\n",
    "\n",
    "        # ============================================\n",
    "        # BENCHMARKS - aligned by tick\n",
    "        # ============================================\n",
    "        first_tick = ticks[0]\n",
    "        last_tick = ticks[-1]\n",
    "\n",
    "        # Create aligned arrays using actual ticks\n",
    "        aligned_ticks = sorted(equity_by_tick.keys())\n",
    "        equity_net = [equity_by_tick[t] for t in aligned_ticks]\n",
    "        equity_gross = [equity_gross_by_tick[t] for t in aligned_ticks]\n",
    "\n",
    "        # Buy & Hold: computed at each tick visited\n",
    "        first_price = all_prices[first_tick]\n",
    "        buy_hold = [all_prices[t] / first_price for t in aligned_ticks]\n",
    "        short_hold = [first_price / all_prices[t] for t in aligned_ticks]\n",
    "\n",
    "        # Price segment at each tick\n",
    "        price_segment = [all_prices[t] for t in aligned_ticks]\n",
    "\n",
    "        # ============================================\n",
    "        # TRADE ANALYSIS\n",
    "        # ============================================\n",
    "        positions_arr = np.array(positions)\n",
    "        position_changes = np.diff(positions_arr)\n",
    "        trade_indices = np.where(position_changes != 0)[0] + 1\n",
    "\n",
    "        # Calculate per-trade returns\n",
    "        trade_returns = []\n",
    "        if len(trade_indices) > 1:\n",
    "            for i in range(len(trade_indices) - 1):\n",
    "                start_idx = trade_indices[i]\n",
    "                end_idx = trade_indices[i + 1]\n",
    "                pos_during = positions_arr[start_idx]\n",
    "\n",
    "                start_price = all_prices[ticks[start_idx]]\n",
    "                end_price = all_prices[ticks[end_idx]]\n",
    "\n",
    "                if pos_during == 1:  # Long\n",
    "                    ret = (end_price / start_price) - 1\n",
    "                elif pos_during == -1:  # Short\n",
    "                    ret = (start_price / end_price) - 1\n",
    "                else:\n",
    "                    ret = 0\n",
    "                trade_returns.append(ret)\n",
    "\n",
    "        trade_returns = np.array(trade_returns)\n",
    "        winning_trades = trade_returns[trade_returns > 0]\n",
    "        losing_trades = trade_returns[trade_returns < 0]\n",
    "\n",
    "        # ============================================\n",
    "        # METRICS\n",
    "        # ============================================\n",
    "        equity_arr = np.array(equity_net)\n",
    "        buy_hold_arr = np.array(buy_hold)\n",
    "        returns = np.diff(np.log(equity_arr + 1e-12))\n",
    "\n",
    "        # Core metrics\n",
    "        total_return = (equity_arr[-1] / equity_arr[0]) - 1\n",
    "        total_return_gross = (equity_gross[-1] / equity_gross[0]) - 1\n",
    "        bh_return = (buy_hold_arr[-1] / buy_hold_arr[0]) - 1\n",
    "        sharpe = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
    "\n",
    "        # Drawdown\n",
    "        running_max = np.maximum.accumulate(equity_arr)\n",
    "        drawdowns = (running_max - equity_arr) / running_max\n",
    "        max_dd = np.max(drawdowns)\n",
    "\n",
    "        # Calmar ratio\n",
    "        calmar = (total_return * 252 / len(returns)) / (max_dd + 1e-8) if max_dd > 0 else 0\n",
    "\n",
    "        # Trade metrics\n",
    "        n_trades = len(trade_returns)\n",
    "        win_rate = len(winning_trades) / n_trades * 100 if n_trades > 0 else 0\n",
    "        avg_win = np.mean(winning_trades) * 100 if len(winning_trades) > 0 else 0\n",
    "        avg_loss = np.mean(losing_trades) * 100 if len(losing_trades) > 0 else 0\n",
    "        profit_factor = abs(np.sum(winning_trades) / np.sum(losing_trades)) if len(losing_trades) > 0 and np.sum(losing_trades) != 0 else np.inf\n",
    "\n",
    "        # Position analysis\n",
    "        long_pct = np.sum(positions_arr == 1) / len(positions_arr) * 100\n",
    "        short_pct = np.sum(positions_arr == -1) / len(positions_arr) * 100\n",
    "        flat_pct = np.sum(positions_arr == 0) / len(positions_arr) * 100\n",
    "\n",
    "        # ============================================\n",
    "        # ALIGNMENT CHECK\n",
    "        # ============================================\n",
    "        # Count how often PPO beats Buy & Hold\n",
    "        ppo_vs_bh = equity_arr - buy_hold_arr\n",
    "        ahead_count = np.sum(ppo_vs_bh > 0)\n",
    "        behind_count = np.sum(ppo_vs_bh < 0)\n",
    "\n",
    "        # ============================================\n",
    "        # PRINT RESULTS\n",
    "        # ============================================\n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(f\"EPISODE {ep+1} RESULTS\")\n",
    "        print(f\"{'─'*60}\")\n",
    "\n",
    "        print(f\"\\n📊 RETURNS\")\n",
    "        print(f\"   Total Return (net):    {total_return*100:+.2f}%\")\n",
    "        print(f\"   Total Return (gross):  {total_return_gross*100:+.2f}%\")\n",
    "        print(f\"   Buy & Hold Return:     {bh_return*100:+.2f}%\")\n",
    "        print(f\"   Short & Hold Return:   {(short_hold[-1]-1)*100:+.2f}%\")\n",
    "        print(f\"   Outperformance vs B&H: {(total_return - bh_return)*100:+.2f}%\")\n",
    "\n",
    "        print(f\"\\n📉 ALIGNMENT CHECK\")\n",
    "        print(f\"   Steps where PPO > B&H: {ahead_count} ({100*ahead_count/len(equity_arr):.1f}%)\")\n",
    "        print(f\"   Steps where PPO < B&H: {behind_count} ({100*behind_count/len(equity_arr):.1f}%)\")\n",
    "\n",
    "        print(f\"\\n📈 RISK METRICS\")\n",
    "        print(f\"   Sharpe Ratio (ann.):   {sharpe:.3f}\")\n",
    "        print(f\"   Max Drawdown:          {max_dd*100:.2f}%\")\n",
    "        print(f\"   Calmar Ratio:          {calmar:.3f}\")\n",
    "\n",
    "        print(f\"\\n🔄 TRADE ANALYSIS\")\n",
    "        print(f\"   Total Trades:          {n_trades}\")\n",
    "        print(f\"   Win Rate:              {win_rate:.1f}%\")\n",
    "        print(f\"   Avg Winning Trade:     {avg_win:+.2f}%\")\n",
    "        print(f\"   Avg Losing Trade:      {avg_loss:+.2f}%\")\n",
    "        print(f\"   Profit Factor:         {profit_factor:.2f}\")\n",
    "\n",
    "        print(f\"\\n⚖️  POSITION DISTRIBUTION\")\n",
    "        print(f\"   Long:  {long_pct:5.1f}%  {'█' * int(long_pct/5)}\")\n",
    "        print(f\"   Short: {short_pct:5.1f}%  {'█' * int(short_pct/5)}\")\n",
    "        print(f\"   Flat:  {flat_pct:5.1f}%  {'█' * int(flat_pct/5)}\")\n",
    "\n",
    "        # ============================================\n",
    "        # PLOT 1: EQUITY CURVES (properly aligned)\n",
    "        # ============================================\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "        x_axis = np.arange(len(aligned_ticks))\n",
    "\n",
    "        # Panel 1: Price + Equity\n",
    "        ax1 = axes[0]\n",
    "        ax1_twin = ax1.twinx()\n",
    "\n",
    "        ax1.plot(x_axis, price_segment, color=\"blue\", alpha=0.4, linewidth=1, label=\"Price\")\n",
    "        ax1.set_ylabel(\"Price\", color=\"blue\")\n",
    "        ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "        ax1_twin.plot(x_axis, equity_net, color=\"green\", linewidth=2, label=\"PPO (net)\")\n",
    "        ax1_twin.plot(x_axis, equity_gross, color=\"lightgreen\", linewidth=1, linestyle=\"--\", label=\"PPO (gross)\", alpha=0.7)\n",
    "        ax1_twin.plot(x_axis, buy_hold, color=\"gray\", linewidth=1.5, linestyle=\"--\", label=\"Buy & Hold\")\n",
    "        ax1_twin.plot(x_axis, short_hold, color=\"red\", linewidth=1, linestyle=\":\", alpha=0.5, label=\"Short & Hold\")\n",
    "        ax1_twin.axhline(y=1.0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "        ax1_twin.set_ylabel(\"Equity\", color=\"green\")\n",
    "        ax1_twin.tick_params(axis=\"y\", labelcolor=\"green\")\n",
    "        ax1_twin.legend(loc=\"upper left\")\n",
    "\n",
    "        ax1.set_title(\"Equity Curves vs Benchmarks (Aligned by Tick)\", fontsize=12, fontweight=\"bold\")\n",
    "        ax1.grid(alpha=0.3)\n",
    "\n",
    "        # Panel 2: Drawdown\n",
    "        ax2 = axes[1]\n",
    "        ax2.fill_between(x_axis, 0, -drawdowns*100, color=\"red\", alpha=0.3)\n",
    "        ax2.plot(x_axis, -drawdowns*100, color=\"red\", linewidth=1)\n",
    "        ax2.axhline(y=-max_dd*100, color=\"darkred\", linestyle=\"--\", label=f\"Max DD: {max_dd*100:.1f}%\")\n",
    "        ax2.set_ylabel(\"Drawdown (%)\")\n",
    "        ax2.set_title(\"Underwater Curve (Drawdown)\", fontsize=12, fontweight=\"bold\")\n",
    "        ax2.legend(loc=\"lower left\")\n",
    "        ax2.grid(alpha=0.3)\n",
    "\n",
    "        # Panel 3: Position over time with trade markers\n",
    "        ax3 = axes[2]\n",
    "\n",
    "        # Color the background based on position\n",
    "        for i in range(len(x_axis) - 1):\n",
    "            if i < len(positions):\n",
    "                if positions[i] == 1:\n",
    "                    ax3.axvspan(i, i+1, alpha=0.3, color=\"green\")\n",
    "                elif positions[i] == -1:\n",
    "                    ax3.axvspan(i, i+1, alpha=0.3, color=\"red\")\n",
    "\n",
    "        ax3.plot(x_axis, price_segment, color=\"blue\", linewidth=1, alpha=0.7)\n",
    "\n",
    "        # Mark trades\n",
    "        for idx in trade_indices:\n",
    "            if idx < len(x_axis):\n",
    "                marker = \"^\" if positions[idx] == 1 else \"v\"\n",
    "                color = \"green\" if positions[idx] == 1 else \"red\"\n",
    "                ax3.scatter(idx, price_segment[idx], marker=marker, color=color, s=50, zorder=5)\n",
    "\n",
    "        ax3.set_ylabel(\"Price\")\n",
    "        ax3.set_xlabel(\"Step\")\n",
    "        ax3.set_title(\"Position & Trade Markers (▲=Long, ▼=Short)\", fontsize=12, fontweight=\"bold\")\n",
    "        ax3.grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # ============================================\n",
    "        # PLOT 2: TRADE RETURN DISTRIBUTION\n",
    "        # ============================================\n",
    "        if len(trade_returns) > 0:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "            # Histogram\n",
    "            ax1 = axes[0]\n",
    "            ax1.hist(trade_returns * 100, bins=30, color=\"steelblue\", edgecolor=\"white\", alpha=0.7)\n",
    "            ax1.axvline(x=0, color=\"black\", linestyle=\"--\")\n",
    "            ax1.axvline(x=np.mean(trade_returns)*100, color=\"green\", linestyle=\"-\", label=f\"Mean: {np.mean(trade_returns)*100:.2f}%\")\n",
    "            ax1.set_xlabel(\"Trade Return (%)\")\n",
    "            ax1.set_ylabel(\"Frequency\")\n",
    "            ax1.set_title(\"Trade Return Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "            ax1.legend()\n",
    "            ax1.grid(alpha=0.3)\n",
    "\n",
    "            # Cumulative trade returns\n",
    "            ax2 = axes[1]\n",
    "            cum_trade_returns = np.cumprod(1 + trade_returns)\n",
    "            ax2.plot(cum_trade_returns, color=\"green\", linewidth=2)\n",
    "            ax2.axhline(y=1.0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "            ax2.set_xlabel(\"Trade #\")\n",
    "            ax2.set_ylabel(\"Cumulative Return\")\n",
    "            ax2.set_title(\"Cumulative Returns by Trade\", fontsize=12, fontweight=\"bold\")\n",
    "            ax2.grid(alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION COMPLETE\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        \"total_return\": total_return,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"max_drawdown\": max_dd,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"profit_factor\": profit_factor,\n",
    "        \"n_trades\": n_trades,\n",
    "        \"equity_curve\": equity_net,\n",
    "        \"buy_hold\": buy_hold,\n",
    "        \"positions\": positions,\n",
    "        \"ticks\": aligned_ticks,\n",
    "        \"ppo_ahead_pct\": 100*ahead_count/len(equity_arr),\n",
    "        \"ppo_behind_pct\": 100*behind_count/len(equity_arr)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ys25kj8mgqk",
   "metadata": {},
   "source": [
    "## 13. Final Evaluation (Test Data)\n",
    "\n",
    "**The real test**: Evaluate on held-out data the model has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40455945",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = DummyVecEnv([make_test_env()])\n",
    "test_env = VecNormalize.load(\"vec_normalize.pkl\", test_env)\n",
    "test_env.training = False\n",
    "test_env.norm_reward = False\n",
    "\n",
    "model = PPO.load(\"ppo_trading_final\")\n",
    "\n",
    "results = evaluate_agent(model, test_env, episodes=1)\n",
    "\n",
    "# Show summary of results returned\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETURNED RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Return: {results['total_return']*100:+.2f}%\")\n",
    "print(f\"Sharpe Ratio: {results['sharpe']:.3f}\")\n",
    "print(f\"Max Drawdown: {results['max_drawdown']*100:.2f}%\")\n",
    "print(f\"Win Rate: {results['win_rate']:.1f}%\")\n",
    "print(f\"PPO ahead of B&H: {results['ppo_ahead_pct']:.1f}% of steps\")\n",
    "print(f\"PPO behind B&H: {results['ppo_behind_pct']:.1f}% of steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h2ibmr6f0uu",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Feature Selection (Ablation Study)\n",
    "\n",
    "Systematically test which feature groups help vs hurt performance:\n",
    "1. Train baseline with all features\n",
    "2. Remove each group one-by-one and compare\n",
    "3. Test each group in isolation\n",
    "\n",
    "**Interpretation**:\n",
    "- If removing a group **improves** performance → that group is **harmful**\n",
    "- If removing a group **hurts** performance → that group is **helpful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d262fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE SELECTION & HYPERPARAMETER SEARCH\n",
    "# ============================================\n",
    "# This framework helps identify which features help vs hurt performance\n",
    "\n",
    "import itertools\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class FeatureSelector:\n",
    "    \"\"\"\n",
    "    Ablation study for features: train models with different feature subsets\n",
    "    and compare performance to identify helpful vs harmful features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, prices, window_size=30, train_ratio=0.8):\n",
    "        self.df = df\n",
    "        self.prices = prices\n",
    "        self.window_size = window_size\n",
    "        self.train_ratio = train_ratio\n",
    "\n",
    "        self.all_features = list(df.columns.drop(\"target_close\"))\n",
    "        self.results = []\n",
    "\n",
    "        # Define feature groups for structured ablation\n",
    "        self.feature_groups = self._create_feature_groups()\n",
    "\n",
    "    def _create_feature_groups(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Group features by type for systematic testing.\"\"\"\n",
    "        groups = {\n",
    "            'price_ohlcv': [f for f in self.all_features if any(x in f for x in ['Open', 'High', 'Low', 'Close', 'Volume']) and 't-1' in f],\n",
    "            'returns': [f for f in self.all_features if 'Return' in f],\n",
    "            'momentum': [f for f in self.all_features if any(x in f for x in ['RSI', 'Stoch', 'SMA'])],\n",
    "            'volatility': [f for f in self.all_features if 'Entropy' in f],\n",
    "            'cross_ticker': [f for f in self.all_features if 'Ratio' in f or 'XPH' in f],\n",
    "            'calendar': [f for f in self.all_features if any(x in f for x in ['day_of', 'month', 'quarter'])],\n",
    "            'macro': [f for f in self.all_features if any(x in f for x in ['cpi', 'nfp', 'holiday'])],\n",
    "        }\n",
    "\n",
    "        # Remove empty groups and show\n",
    "        groups = {k: v for k, v in groups.items() if v}\n",
    "\n",
    "        print(\"Feature Groups Detected:\")\n",
    "        for name, features in groups.items():\n",
    "            print(f\"  {name}: {len(features)} features\")\n",
    "\n",
    "        return groups\n",
    "\n",
    "    def run_single_experiment(\n",
    "        self,\n",
    "        feature_subset: List[str],\n",
    "        experiment_name: str,\n",
    "        timesteps: int = 50000,\n",
    "        reward_config: dict = None\n",
    "    ) -> Dict:\n",
    "        \"\"\"Train a model on a feature subset and return metrics.\"\"\"\n",
    "\n",
    "        if reward_config is None:\n",
    "            reward_config = REWARD_CONFIG.copy()\n",
    "\n",
    "        # Prepare data\n",
    "        signal_features_subset = self.df[feature_subset].values.astype(np.float32)\n",
    "\n",
    "        total_len = len(self.df)\n",
    "        train_end = int(total_len * self.train_ratio)\n",
    "        train_bounds = (self.window_size, train_end)\n",
    "        test_bounds = (train_end, total_len)\n",
    "\n",
    "        # Create environments — pass only supported constructor args and\n",
    "        # apply extra reward-shaping keys to the env instance if present.\n",
    "        def make_env(bounds):\n",
    "            def _init():\n",
    "                env = FlexibleTradingEnv(\n",
    "                    df=self.df,\n",
    "                    prices=self.prices,\n",
    "                    signal_features=signal_features_subset,\n",
    "                    window_size=self.window_size,\n",
    "                    frame_bound=bounds,\n",
    "                    include_position_in_obs=True,\n",
    "                    fee=reward_config.get('fee', 0.0005),\n",
    "                    holding_cost=reward_config.get('holding_cost', 0.0),\n",
    "                    short_borrow_cost=reward_config.get('short_borrow_cost', 0.0),\n",
    "                )\n",
    "                # Apply additional shaping params only if the env supports them\n",
    "                for k, v in reward_config.items():\n",
    "                    if hasattr(env, k):\n",
    "                        setattr(env, k, v)\n",
    "                return env\n",
    "            return _init\n",
    "\n",
    "        train_env = DummyVecEnv([make_env(train_bounds)])\n",
    "        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "        # Quick training\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=1024,\n",
    "            batch_size=64,\n",
    "            gamma=0.95,\n",
    "            ent_coef=0.05,\n",
    "            verbose=0,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        # Evaluate on test\n",
    "        test_env = DummyVecEnv([make_env(test_bounds)])\n",
    "        test_env = VecNormalize(test_env, training=False, norm_obs=True, norm_reward=False)\n",
    "        test_env.obs_rms = train_env.obs_rms\n",
    "\n",
    "        # Run evaluation\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        equity = 1.0\n",
    "        positions = []\n",
    "        last_pos = 0\n",
    "        base_env = test_env.venv.envs[0].unwrapped\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, info = test_env.step(action)\n",
    "            done = bool(done[0])\n",
    "\n",
    "            tick = info[0][\"tick\"]\n",
    "            pos = info[0][\"position\"]\n",
    "            positions.append(pos)\n",
    "\n",
    "            if len(positions) > 1:\n",
    "                price_ratio = self.prices[tick] / self.prices[tick - 1]\n",
    "                equity *= price_ratio ** last_pos\n",
    "            last_pos = pos\n",
    "\n",
    "        # Compute metrics\n",
    "        positions = np.array(positions)\n",
    "        n_trades = np.sum(np.diff(positions) != 0)\n",
    "\n",
    "        result = {\n",
    "            'experiment': experiment_name,\n",
    "            'n_features': len(feature_subset),\n",
    "            'features': feature_subset,\n",
    "            'test_return': (equity - 1) * 100,\n",
    "            'n_trades': int(n_trades),\n",
    "            'timesteps': timesteps\n",
    "        }\n",
    "\n",
    "        self.results.append(result)\n",
    "        return result\n",
    "\n",
    "    def ablation_study(self, timesteps: int = 50000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run ablation study: test removing each feature group one at a time.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ABLATION STUDY: Testing feature group importance\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Baseline: all features\n",
    "        print(\"\\n[1/N] Training BASELINE (all features)...\")\n",
    "        baseline = self.run_single_experiment(\n",
    "            self.all_features,\n",
    "            \"BASELINE_all_features\",\n",
    "            timesteps\n",
    "        )\n",
    "        print(f\"    Return: {baseline['test_return']:+.2f}%, Trades: {baseline['n_trades']}\")\n",
    "\n",
    "        # Remove each group one at a time\n",
    "        for i, (group_name, group_features) in enumerate(self.feature_groups.items()):\n",
    "            remaining = [f for f in self.all_features if f not in group_features]\n",
    "\n",
    "            if len(remaining) == 0:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n[{i+2}/N] Training WITHOUT {group_name} ({len(group_features)} features)...\")\n",
    "            result = self.run_single_experiment(\n",
    "                remaining,\n",
    "                f\"without_{group_name}\",\n",
    "                timesteps\n",
    "            )\n",
    "\n",
    "            diff = result['test_return'] - baseline['test_return']\n",
    "            impact = \"HELPFUL\" if diff < 0 else \"HARMFUL\" if diff > 0.5 else \"NEUTRAL\"\n",
    "            print(f\"    Return: {result['test_return']:+.2f}% (diff: {diff:+.2f}%) → {group_name} is {impact}\")\n",
    "\n",
    "        # Single group tests\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"Testing individual feature groups...\")\n",
    "\n",
    "        for i, (group_name, group_features) in enumerate(self.feature_groups.items()):\n",
    "            if len(group_features) == 0:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n[Single] Training with ONLY {group_name}...\")\n",
    "            result = self.run_single_experiment(\n",
    "                group_features,\n",
    "                f\"only_{group_name}\",\n",
    "                timesteps\n",
    "            )\n",
    "            print(f\"    Return: {result['test_return']:+.2f}%, Trades: {result['n_trades']}\")\n",
    "\n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "    def plot_results(self):\n",
    "        \"\"\"Visualize ablation study results.\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to plot. Run ablation_study() first.\")\n",
    "            return\n",
    "\n",
    "        df_results = pd.DataFrame(self.results)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # Bar chart of returns\n",
    "        ax1 = axes[0]\n",
    "        colors = ['green' if r > 0 else 'red' for r in df_results['test_return']]\n",
    "        bars = ax1.barh(df_results['experiment'], df_results['test_return'], color=colors, alpha=0.7)\n",
    "        ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax1.set_xlabel('Test Return (%)')\n",
    "        ax1.set_title('Test Returns by Feature Configuration')\n",
    "        ax1.grid(alpha=0.3, axis='x')\n",
    "\n",
    "        # Highlight baseline\n",
    "        baseline_idx = df_results[df_results['experiment'].str.contains('BASELINE')].index\n",
    "        if len(baseline_idx) > 0:\n",
    "            baseline_val = df_results.loc[baseline_idx[0], 'test_return']\n",
    "            ax1.axvline(x=baseline_val, color='blue', linestyle='--', label='Baseline')\n",
    "            ax1.legend()\n",
    "\n",
    "        # Trade count\n",
    "        ax2 = axes[1]\n",
    "        ax2.barh(df_results['experiment'], df_results['n_trades'], color='steelblue', alpha=0.7)\n",
    "        ax2.set_xlabel('Number of Trades')\n",
    "        ax2.set_title('Trade Activity by Configuration')\n",
    "        ax2.grid(alpha=0.3, axis='x')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return df_results\n",
    "\n",
    "\n",
    "# Create selector instance\n",
    "print(\"Initializing Feature Selector...\")\n",
    "feature_selector = FeatureSelector(df, prices, window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fno3ppvoila",
   "metadata": {},
   "source": [
    "### Run Ablation Study\n",
    "\n",
    "⚠️ **Takes ~10-30 minutes** depending on `timesteps`. Use 50k for quick iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run (takes 10-30 min)\n",
    "ablation_results = feature_selector.ablation_study(timesteps=50000)\n",
    "feature_selector.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2wtwwacsgsx",
   "metadata": {},
   "source": [
    "## 15. Hyperparameter Search\n",
    "\n",
    "Random search over reward shaping and PPO hyperparameters.\n",
    "\n",
    "Searches over:\n",
    "- `reward_scaling`: [10, 50, 100, 200]\n",
    "- `trade_penalty`: [0, 0.0005, 0.001, 0.002]\n",
    "- `profit_bonus`: [0, 0.25, 0.5, 1.0]\n",
    "- `ent_coef`: [0.01, 0.05, 0.1]\n",
    "- `gamma`: [0.9, 0.95, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4102e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HYPERPARAMETER GRID SEARCH\n",
    "# ============================================\n",
    "# Test different reward shaping configurations\n",
    "\n",
    "def hyperparameter_search(feature_subset=None, n_trials=10, timesteps=50000):\n",
    "    \"\"\"\n",
    "    Random search over reward shaping hyperparameters.\n",
    "    \"\"\"\n",
    "    if feature_subset is None:\n",
    "        feature_subset = feature_cols.tolist()\n",
    "\n",
    "    signal_features_search = df[feature_subset].values.astype(np.float32)\n",
    "\n",
    "    # Hyperparameter search space\n",
    "    param_space = {\n",
    "        'reward_scaling': [10, 50, 100, 200],\n",
    "        'trade_penalty': [0.0, 0.0005, 0.001, 0.002],\n",
    "        'profit_bonus': [0.0, 0.25, 0.5, 1.0],\n",
    "        'trend_following_bonus': [0.0, 0.0001, 0.0005],\n",
    "        'ent_coef': [0.01, 0.05, 0.1],\n",
    "        'gamma': [0.9, 0.95, 0.99],\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"HYPERPARAMETER SEARCH ({n_trials} trials)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        # Random sample from parameter space\n",
    "        params = {k: np.random.choice(v) for k, v in param_space.items()}\n",
    "\n",
    "        reward_config = {\n",
    "            'fee': 0.0005,\n",
    "            'holding_cost': 0.0,\n",
    "            'short_borrow_cost': 0.0,\n",
    "            'reward_scaling': params['reward_scaling'],\n",
    "            'trade_penalty': params['trade_penalty'],\n",
    "            'profit_bonus': params['profit_bonus'],\n",
    "            'trend_following_bonus': params['trend_following_bonus'],\n",
    "        }\n",
    "\n",
    "        # Create environment\n",
    "        def make_env(bounds):\n",
    "            def _init():\n",
    "                return FlexibleTradingEnv(\n",
    "                    df=df,\n",
    "                    prices=prices,\n",
    "                    signal_features=signal_features_search,\n",
    "                    window_size=window_size,\n",
    "                    frame_bound=bounds,\n",
    "                    include_position_in_obs=True,\n",
    "                    **reward_config\n",
    "                )\n",
    "            return _init\n",
    "\n",
    "        train_env = DummyVecEnv([make_env(train_frame_bound)])\n",
    "        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "\n",
    "        # Train model\n",
    "        model = PPO(\n",
    "            \"MlpPolicy\",\n",
    "            train_env,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=1024,\n",
    "            batch_size=64,\n",
    "            gamma=params['gamma'],\n",
    "            ent_coef=params['ent_coef'],\n",
    "            verbose=0,\n",
    "            seed=trial\n",
    "        )\n",
    "\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        # Evaluate\n",
    "        test_env = DummyVecEnv([make_env(test_frame_bound)])\n",
    "        test_env = VecNormalize(test_env, training=False, norm_obs=True, norm_reward=False)\n",
    "        test_env.obs_rms = train_env.obs_rms\n",
    "\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        equity = 1.0\n",
    "        n_trades = 0\n",
    "        last_pos = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, done, info = test_env.step(action)\n",
    "            done = bool(done[0])\n",
    "\n",
    "            tick = info[0][\"tick\"]\n",
    "            pos = info[0][\"position\"]\n",
    "\n",
    "            if pos != last_pos:\n",
    "                n_trades += 1\n",
    "\n",
    "            price_ratio = prices[tick] / prices[tick - 1]\n",
    "            equity *= price_ratio ** last_pos\n",
    "            last_pos = pos\n",
    "\n",
    "        test_return = (equity - 1) * 100\n",
    "\n",
    "        results.append({\n",
    "            'trial': trial,\n",
    "            'test_return': test_return,\n",
    "            'n_trades': n_trades,\n",
    "            **params\n",
    "        })\n",
    "\n",
    "        print(f\"Trial {trial+1}/{n_trials}: Return={test_return:+.2f}%, Trades={n_trades}\")\n",
    "        print(f\"  Params: scaling={params['reward_scaling']}, penalty={params['trade_penalty']}, \"\n",
    "              f\"bonus={params['profit_bonus']}, gamma={params['gamma']}, ent={params['ent_coef']}\")\n",
    "\n",
    "    # Find best\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_idx = results_df['test_return'].idxmax()\n",
    "    best = results_df.loc[best_idx]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEST CONFIGURATION:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Test Return: {best['test_return']:+.2f}%\")\n",
    "    print(f\"  Trades: {best['n_trades']}\")\n",
    "    print(f\"  Parameters:\")\n",
    "    for k in param_space.keys():\n",
    "        print(f\"    {k}: {best[k]}\")\n",
    "\n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, param in enumerate(['reward_scaling', 'trade_penalty', 'profit_bonus', 'gamma', 'ent_coef']):\n",
    "        ax = axes[i]\n",
    "        ax.scatter(results_df[param], results_df['test_return'], alpha=0.6)\n",
    "        ax.set_xlabel(param)\n",
    "        ax.set_ylabel('Test Return (%)')\n",
    "        ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    axes[-1].hist(results_df['test_return'], bins=15, color='steelblue', alpha=0.7)\n",
    "    axes[-1].set_xlabel('Test Return (%)')\n",
    "    axes[-1].set_ylabel('Frequency')\n",
    "    axes[-1].set_title('Return Distribution')\n",
    "\n",
    "    plt.suptitle('Hyperparameter Search Results', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df, best\n",
    "\n",
    "# Run hyperparameter search (uncomment to run)\n",
    "# hp_results, best_params = hyperparameter_search(n_trials=20, timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h14goj30xz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Troubleshooting Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e6ee6",
   "metadata": {},
   "source": [
    "| Symptom | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Always same position | Reward too sparse | Increase `reward_scaling` |\n",
    "| Too many trades | No trade penalty | Increase `trade_penalty` |\n",
    "| Overfits train | Too complex features | Remove weak feature groups |\n",
    "| Random behavior | Features not predictive | Check diagnostic cell |\n",
    "| Worse than B&H | Bad market regime | Try different date ranges |\n",
    "\n",
    "**Next steps if still failing:**\n",
    "- Try A2C or SAC algorithms\n",
    "- Add LSTM policy for temporal patterns\n",
    "- Increase data (more tickers, longer history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StockProphet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
