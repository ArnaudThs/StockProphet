import requests
import sqlite3
import time
from datetime import datetime, timedelta
import json

class PolygonNewsCollector:
    def __init__(self, api_key, db_path='stock_news.db'):
        self.api_key = api_key
        self.db_path = db_path
        self.base_url = "https://api.polygon.io/v2/reference/news"
        self.init_database()

    def init_database(self):
        """Initialise la base de donn√©es avec les tables n√©cessaires"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Table pour les articles
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_articles (
                id TEXT PRIMARY KEY,
                ticker TEXT NOT NULL,
                published_utc TEXT,
                title TEXT,
                author TEXT,
                article_url TEXT,
                description TEXT,
                keywords TEXT,
                image_url TEXT,
                sentiment TEXT,
                sentiment_reasoning TEXT,
                raw_json TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # Table pour tracker les tickers et p√©riodes collect√©es
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS collection_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT,
                start_date TEXT,
                end_date TEXT,
                articles_collected INTEGER,
                collected_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # Index pour am√©liorer les performances
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_ticker_date
            ON news_articles(ticker, published_utc)
        ''')

        conn.commit()
        conn.close()
        print(f"‚úÖ Base de donn√©es initialis√©e: {self.db_path}")

    def fetch_news(self, ticker, start_date, end_date, limit=1000):
        """R√©cup√®re les news pour un ticker sur une p√©riode donn√©e"""
        articles = []
        params = {
            'ticker': ticker,
            'published_utc.gte': start_date,
            'published_utc.lte': end_date,
            'limit': min(limit, 1000),  # Max 1000 par requ√™te
            'apiKey': self.api_key,
            'sort': 'published_utc',
            'order': 'asc'
        }

        next_url = None
        total_fetched = 0

        while True:
            try:
                if next_url:
                    url = next_url
                    response = requests.get(url)
                else:
                    response = requests.get(self.base_url, params=params)

                if response.status_code == 429:
                    print("‚ö†Ô∏è  Rate limit atteint, attente de 60 secondes...")
                    time.sleep(60)
                    continue

                response.raise_for_status()
                data = response.json()

                results = data.get('results', [])
                articles.extend(results)
                total_fetched += len(results)

                print(f"   R√©cup√©r√© {len(results)} articles (total: {total_fetched})")

                # V√©rifier s'il y a une page suivante
                next_url = data.get('next_url')
                if not next_url or total_fetched >= limit:
                    break

                # Ajouter l'API key au next_url
                if '?' in next_url:
                    next_url += f'&apiKey={self.api_key}'
                else:
                    next_url += f'?apiKey={self.api_key}'

                # Respecter le rate limit (5 requ√™tes/minute pour free tier)
                time.sleep(12)

            except requests.exceptions.RequestException as e:
                print(f"‚ùå Erreur lors de la requ√™te: {e}")
                break

        return articles

    def save_articles(self, articles, ticker):
        """Sauvegarde les articles dans la base de donn√©es"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        new_articles = 0
        duplicates = 0

        for article in articles:
            try:
                # Extraire les donn√©es avec gestion des valeurs manquantes
                article_id = article.get('id', '')
                published_utc = article.get('published_utc', '')
                title = article.get('title', '')
                author = article.get('author', '')
                article_url = article.get('article_url', '')
                description = article.get('description', '')

                # G√©rer les keywords (liste -> string)
                keywords = json.dumps(article.get('keywords', []))

                # G√©rer l'image
                image_url = article.get('image_url', '')

                # Extraire les donn√©es de sentiment
                insights = article.get('insights', [{}])
                sentiment = ''
                sentiment_reasoning = ''

                if insights and len(insights) > 0:
                    sentiment = insights[0].get('sentiment', '')
                    sentiment_reasoning = insights[0].get('sentiment_reasoning', '')

                # Sauvegarder le JSON complet
                raw_json = json.dumps(article)

                cursor.execute('''
                    INSERT OR IGNORE INTO news_articles
                    (id, ticker, published_utc, title, author, article_url,
                     description, keywords, image_url, sentiment,
                     sentiment_reasoning, raw_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (article_id, ticker, published_utc, title, author,
                      article_url, description, keywords, image_url,
                      sentiment, sentiment_reasoning, raw_json))

                if cursor.rowcount > 0:
                    new_articles += 1
                else:
                    duplicates += 1

            except Exception as e:
                print(f"‚ö†Ô∏è  Erreur lors de la sauvegarde d'un article: {e}")
                continue

        conn.commit()
        conn.close()

        print(f"   üíæ {new_articles} nouveaux articles sauvegard√©s, {duplicates} doublons ignor√©s")
        return new_articles

    def log_collection(self, ticker, start_date, end_date, count):
        """Enregistre la collection dans le log"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO collection_log (ticker, start_date, end_date, articles_collected)
            VALUES (?, ?, ?, ?)
        ''', (ticker, start_date, end_date, count))

        conn.commit()
        conn.close()

    def collect_ticker_history(self, ticker, years=5):
        """Collecte l'historique complet pour un ticker"""
        print(f"\nüìä Collecte des donn√©es pour {ticker} sur {years} ans")

        end_date = datetime.now()
        start_date = end_date - timedelta(days=years*365)

        start_str = start_date.strftime('%Y-%m-%d')
        end_str = end_date.strftime('%Y-%m-%d')

        print(f"   P√©riode: {start_str} √† {end_str}")

        articles = self.fetch_news(ticker, start_str, end_str)

        if articles:
            count = self.save_articles(articles, ticker)
            self.log_collection(ticker, start_str, end_str, count)
            return count

        return 0

    def collect_multiple_tickers(self, tickers, years=5):
        """Collecte les donn√©es pour plusieurs tickers"""
        print(f"\nüöÄ D√©but de la collecte pour {len(tickers)} tickers")
        print("=" * 60)

        results = {}

        for i, ticker in enumerate(tickers, 1):
            print(f"\n[{i}/{len(tickers)}] Traitement de {ticker}")
            try:
                count = self.collect_ticker_history(ticker, years)
                results[ticker] = count
                print(f"‚úÖ {ticker}: {count} articles collect√©s")
            except Exception as e:
                print(f"‚ùå Erreur pour {ticker}: {e}")
                results[ticker] = 0

            # Pause entre les tickers pour √©viter le rate limit
            if i < len(tickers):
                print(f"   ‚è≥ Pause de 15 secondes avant le prochain ticker...")
                time.sleep(15)

        print("\n" + "=" * 60)
        print("üìà R√âSUM√â DE LA COLLECTE")
        print("=" * 60)
        for ticker, count in results.items():
            print(f"   {ticker}: {count} articles")
        print(f"\n   TOTAL: {sum(results.values())} articles collect√©s")

        return results

    def get_stats(self):
        """Affiche les statistiques de la base de donn√©es"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Total d'articles
        cursor.execute("SELECT COUNT(*) FROM news_articles")
        total = cursor.fetchone()[0]

        # Par ticker
        cursor.execute("""
            SELECT ticker, COUNT(*) as count
            FROM news_articles
            GROUP BY ticker
            ORDER BY count DESC
        """)
        by_ticker = cursor.fetchall()

        # Articles avec sentiment
        cursor.execute("""
            SELECT COUNT(*)
            FROM news_articles
            WHERE sentiment != '' AND sentiment IS NOT NULL
        """)
        with_sentiment = cursor.fetchone()[0]

        conn.close()

        print("\nüìä STATISTIQUES DE LA BASE DE DONN√âES")
        print("=" * 60)
        print(f"Total d'articles: {total}")
        print(f"Articles avec sentiment: {with_sentiment} ({with_sentiment/total*100:.1f}%)" if total > 0 else "Articles avec sentiment: 0")
        print("\nPar ticker:")
        for ticker, count in by_ticker:
            print(f"   {ticker}: {count} articles")
        print("=" * 60)


# EXEMPLE D'UTILISATION STANDALONE
if __name__ == "__main__":
    # Configuration
    API_KEY = "SiV7GQdKTF2ZtrAr1xNSrnNYP11dKCAC"

    # Liste de tickers √† collecter
    TICKERS = [
        'AAPL',   # Apple
        'MSFT',   # Microsoft
        'GOOGL',  # Alphabet
        'AMZN',   # Amazon
        'TSLA',   # Tesla
        'META',   # Meta
        'NVDA',   # Nvidia
        'JPM',    # JPMorgan
        'V',      # Visa
        'PPH'     # PPH
    ]

    # Cr√©er le collecteur
    collector = PolygonNewsCollector(api_key=API_KEY)

    # Collecter les donn√©es pour tous les tickers (5 ans par d√©faut)
    results = collector.collect_multiple_tickers(TICKERS, years=5)

    # Afficher les statistiques
    collector.get_stats()

    print("\n‚úÖ Collecte termin√©e! Base de donn√©es pr√™te pour l'entra√Ænement.")
    print(f"üìÅ Fichier: stock_news.db")
